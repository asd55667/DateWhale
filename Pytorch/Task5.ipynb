{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 了解知道Dropout原理\n",
    "Dropout首次出现是在AlexNet的论文里, 其灵感来自与Hiton观察到的一种现象, 银行的柜员每隔一段时间就会变更\n",
    "\n",
    "Dropout通常在深层的神经网络中使用, 作用是作为一种正则方法来减轻网络的过拟合, \n",
    "\n",
    "Dropout的原理是, 按设置好的比率随机删掉指定网络层的神经元, 在一定程度上减少了模型的复杂度, \n",
    "\n",
    "每轮训练drop不同的神经元, 最后训练出的模型相当于是进行了模型融合\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout的numpy实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 1. 1. 0.] \n",
      "\n",
      "[[0.71645649 0.         0.05060891 0.61955856 0.        ]\n",
      " [0.88576087 0.         0.39937113 0.53747527 0.        ]\n",
      " [0.53110658 0.         0.2702829  0.76411014 0.        ]\n",
      " [0.98724908 0.         0.29208576 0.31459534 0.        ]\n",
      " [0.622194   0.         0.1566883  0.26401286 0.        ]\n",
      " [0.76566414 0.         0.4378574  0.27853535 0.        ]\n",
      " [0.72641444 0.         0.08246531 0.5653347  0.        ]\n",
      " [0.62880534 0.         0.73748472 0.12773497 0.        ]\n",
      " [0.20516862 0.         0.80561179 0.00392014 0.        ]\n",
      " [0.23672954 0.         0.49349176 0.19032891 0.        ]\n",
      " [0.59615967 0.         0.68799677 0.70462144 0.        ]\n",
      " [0.34684668 0.         0.45034929 0.3083422  0.        ]\n",
      " [0.82270837 0.         0.47790851 0.01770492 0.        ]\n",
      " [0.41069159 0.         0.92817836 0.40919922 0.        ]\n",
      " [0.96195132 0.         0.20167057 0.19196291 0.        ]\n",
      " [0.57686403 0.         0.70863631 0.60936838 0.        ]\n",
      " [0.04163928 0.         0.30326119 0.70660164 0.        ]\n",
      " [0.07818515 0.         0.21610179 0.91970914 0.        ]\n",
      " [0.9355894  0.         0.47813572 0.41246649 0.        ]\n",
      " [0.5483726  0.         0.06765675 0.10820912 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(20190414)\n",
    "\n",
    "N_SAMPLES = 100\n",
    "N_FEATURES = 20\n",
    "N_HIDDEN = 5\n",
    "KEEP_PROB = 0.5\n",
    "\n",
    "# input \n",
    "x = np.random.rand(N_SAMPLES, N_FEATURES)  \n",
    "\n",
    "# param\n",
    "w = np.random.rand(N_FEATURES, N_HIDDEN) \n",
    "b = np.zeros((N_HIDDEN, ))\n",
    "\n",
    "# dropout \n",
    "drop_mask = np.ones((N_HIDDEN,))\n",
    "idx = np.arange(0, N_HIDDEN)\n",
    "random.shuffle(idx)\n",
    "drop_mask[idx[:int(KEEP_PROB*N_HIDDEN)]] = 0\n",
    "print(drop_mask,'\\n')\n",
    "w *= drop_mask\n",
    "print(w)\n",
    "z = x @ w + b\n",
    "a = 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch中实现dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给AlexNet 添加L1, L2正则\n",
    "# 正则化应该是模型每一层的加权参数，而不是每一层的输出\n",
    "\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "t.manual_seed(20190414)\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "batch_size = 32    \n",
    "    \n",
    "net = AlexNet()    \n",
    "optimizer = t.optim.SGD(net.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "x = t.rand((batch_size, )+ (3,224,224), requires_grad=True)\n",
    "y_ = t.ones(batch_size, requires_grad=True)\n",
    "l1, l2 = t.tensor(0).float(), t.tensor(0).float()\n",
    "\n",
    "\n",
    "optimizer.zero_grad()\n",
    "y = net(x)\n",
    "y[y<0]=0\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "loss = criterion(y, y_.detach())\n",
    "\n",
    "for param in net.parameters():\n",
    "    l1 += t.norm(param, 1)\n",
    "    l2 += t.norm(param, 2)\n",
    "\n",
    "loss = loss + l1 + l2\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
